# Copyright (c) 2025 sakib-maho
# Licensed under the MIT License
# See LICENSE file for details

# -*- coding: utf-8 -*-
"""Decision Tree

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1A96X12Ne9fzGQR4_PozHHAGEIVkw699c
"""

import pandas as pd

#dataframe
data = pd.read_csv('tic-tac-toe.csv')

print(data.columns)

data

labels = data['Class']
if 'positive' in labels:
  print("Yes")
else:
  print("No")

X = []
for i in data:
  X.append(data.groupby([i, 'Class'], as_index=False)['Class'].count())

X

Y = []
for i in data:
  Y.append(data.groupby([i, i], as_index=False)[i].count())

Y

X[1]['Class']
t = len(X)
for i in range(len(X)-1):
  for j in X[i]['Class']:
    for i in range(2):

temp = -1
flag = -1
flag1 = 0
for i in data:
  temp = temp + 1
  flag1 = 0
  if i != 'Class':
    for j in X[temp]['Class']:
      flag = flag + 1
      if flag == 2:
        flag1 = flag1 + 1
        flag = -1
      X[temp]['Class'] = X[temp]['Class']/Y[temp][i][flag1]

X

f = -0.5*math.log(0.5,2)-(1-0.5)*math.log(1-0.5,2)

for i in X[0]['Class']:
  print(i)

f = -1
for i in range(len(X)-1):
  f = -1
  for j in X[i]['Class']:
    f = f + 1
    X[i]['Class'][f] =  -j*math.log(j,2)-(1-j)*math.log(1-j,2)

X

for i in Y[0]['top-left-square']:
  print(i)

Y[0]['top-left-square'][0]

Y[0]

for i in data:
  print(i)

import numpy as np
import math
import random

class Node:
    def __init__(self, attribute=None, attribute_values=None, child_nodes=None, decision=None):
        self.attribute = attribute
        self.attribute_values = attribute_values
        self.child_nodes = child_nodes
        self.decision = decision


class DecisionTree:

    root = None

    @staticmethod
    def plurality_values(data):
        labels = data[:, data.shape[1] - 1]  # store the last column in labels
        count1 = 0
        count2 = 0
        for label in labels:
          if label == 'positive':
            count1 = count1 + 1
          else:
            count2 = count2 + 1
        if count1 > count2:
          return 'positive'
        else:
          return 'negative'

    @staticmethod
    def all_zero(data):
        labels = data[:, data.shape[1] - 1]  # store the last column in labels
        count1 = 0
        count2 = 0
        for label in labels:
          count2 = count2 + 1
          if 'negative' == label:
            count1 = count1 + 1
        if count1 == count2:
          return True
        else:
          return False

    @staticmethod
    def all_one(data):
        labels = data[:, data.shape[1] - 1]  # store the last column in labels
        count1 = 0
        count2 = 0
        for label in labels:
          count2 = count2 + 1
          if 'positive' == label:
            count1 = count1 + 1
        if count1 == count2:
          return True
        else:
          return False


    @staticmethod
    def importance(data, attributes):
        labels = data[:, data.shape[1] - 1]  # store the last column in labels
        count1 = 0
        count2 = 0
        for label in labels:
          if label == 'positive':
            count1 = count1 + 1
          else:
            count2 = count2 + 1
        
        entropy_yes = -count1*math.log(count1,2)-(1-count1)*math.log(1-count1,2)
        entropy_no = -count2*math.log(count2,2)-(1-count2)*math.log(1-count2,2)

        for attribute in attributes:
          X = []
          Y = []
          X.append(data.groupby([attribute, 'Class'], as_index=False)['Class'].count())
          Y.append(data.groupby([attribute, attribute], as_index=False)[attribute].count())
        temp = -1
        flag = -1
        flag1 = 0
        for i in attributes:
          temp = temp + 1
          flag1 = 0
          if i != 'Class':
            for j in X[temp]['Class']:
              flag = flag + 1
              if flag == 2:
                flag1 = flag1 + 1
                flag = -1
                X[temp]['Class'] = X[temp]['Class']/Y[temp][i][flag1]
        
        for i in range(len(X)-1):
          f = -1
          for j in X[i]['Class']:
            f = f + 1
            X[i]['Class'][f] =  -j*math.log(j,2)-(1-j)*math.log(1-j,2)
        
        for i in range(len(Y)):
          for j in Y[i]:
            Y[i] = entropy_yes - (Y[i]/(Y[0]+Y[1]+Y[2]) * (X[i]['Class'][j])

        max_info = -1

        for i in range(len(Y)):
          for j in Y[i]:
            if max_info < j:
              max_info = j
              attribute1 = Y[i][j]

        return attribute1


    def train(self, data, attributes, parent_data):
        data = np.array(data)
        parent_data = np.array(parent_data)
        attributes = list(attributes)

        if data.shape[0] == 0:  # if x is empty
            return Node(decision=self.plurality_values(parent_data))

        elif self.all_zero(data):
            return Node(decision=0)

        elif self.all_one(data):
            return Node(decision=1)

        elif len(attributes) == 0:
            return Node(decision=self.plurality_values(data))

        else:
            a = self.importance(data, attributes)
            tree = Node(attribute=a, attribute_values=np.unique(data[:, a]), child_nodes=[])
            attributes.remove(a)
            for vk in np.unique(data[:, a]):
                new_data = data[data[:, a] == vk, :]
                subtree = self.train(new_data, attributes, data)
                tree.child_nodes.append(subtree)

            return tree

    def fit(self, data):
        self.root = self.train(data, list(range(data.shape[1] - 1)), np.array([]))

    def predict(self, data):
        predictions = []
        for i in range(data.shape[0]):
            current_node = self.root
            while True:
                if current_node.decision is None:
                    current_attribute = current_node.attribute
                    current_attribute_value = data[i, current_attribute]
                    if current_attribute_value not in current_node.attribute_values:
                        predictions.append(random.randint(0, 1))
                        break
                    idx = list(current_node.attribute_values).index(current_attribute_value)

                    current_node = current_node.child_nodes[idx]
                else:
                    predictions.append(current_node.decision)
                    break

        return predictions